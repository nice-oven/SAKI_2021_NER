{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7yjL03CKOoEk"
   },
   "source": [
    "Getting started with Spacy<br>\n",
    "Import data.<br>\n",
    "We repeat the preprocessing from the previous homework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vOxAAu3cOm9D",
    "outputId": "efdc833a-11fa-47e3-815d-e5ed74d43b8e"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "id": "HOc5LrSCPSEC"
   },
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "id": "jd_X8KyOPWlJ"
   },
   "outputs": [],
   "source": [
    "# os.chdir( \"/content/gdrive/MyDrive/flair\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "id": "NBNxx2f8Pcmq"
   },
   "outputs": [],
   "source": [
    "path_to_data = os.getcwd() + '/Entity Recognition in Resumes.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "id": "dUKwMlfkaR2n"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First line:\t {\"content\": \"Afreen Jamadar\\nActive member of IIIT Committee in Third year\\n\\nSangli, Maharashtra - Email me on Indeed: indeed.com/r/Afreen-Jamadar/8baf379b705e37c6\\n\\nI wish to use my knowledge, skills and conceptual understanding to create excellent team\\nenvironments and work consistently achieving organization objectives believes in taking initiative\\nand work to excellence in my work.\\n\\nWORK EXPERIENCE\\n\\nActive member of IIIT Committee in Third year\\n\\nCisco Networking -  Kanpur, Uttar Pradesh\\n\\norganized by Techkriti IIT Kanpur and Azure Skynet.\\nPERSONALLITY TRAITS:\\n• Quick learning ability\\n• hard working\\n\\nEDUCATION\\n\\nPG-DAC\\n\\nCDAC ACTS\\n\\n2017\\n\\nBachelor of Engg in Information Technology\\n\\nShivaji University Kolhapur -  Kolhapur, Maharashtra\\n\\n2016\\n\\nSKILLS\\n\\nDatabase (Less than 1 year), HTML (Less than 1 year), Linux. (Less than 1 year), MICROSOFT\\nACCESS (Less than 1 year), MICROSOFT WINDOWS (Less than 1 year)\\n\\nADDITIONAL INFORMATION\\n\\nTECHNICAL SKILLS:\\n\\n• Programming Languages: C, C++, Java, .net, php.\\n• Web Designing: HTML, XML\\n• Operating Systems: Windows […] Windows Server 2003, Linux.\\n• Database: MS Access, MS SQL Server 2008, Oracle 10g, MySql.\\n\\nhttps://www.indeed.com/r/Afreen-Jamadar/8baf379b705e37c6?isid=rex-download&ikw=download-top&co=IN\",\"annotation\":[{\"label\":[\"Email Address\"],\"points\":[{\"start\":1155,\"end\":1198,\"text\":\"indeed.com/r/Afreen-Jamadar/8baf379b705e37c6\"}]},{\"label\":[\"Links\"],\"points\":[{\"start\":1143,\"end\":1239,\"text\":\"https://www.indeed.com/r/Afreen-Jamadar/8baf379b705e37c6?isid=rex-download&ikw=download-top&co=IN\"}]},{\"label\":[\"Skills\"],\"points\":[{\"start\":743,\"end\":1140,\"text\":\"Database (Less than 1 year), HTML (Less than 1 year), Linux. (Less than 1 year), MICROSOFT\\nACCESS (Less than 1 year), MICROSOFT WINDOWS (Less than 1 year)\\n\\nADDITIONAL INFORMATION\\n\\nTECHNICAL SKILLS:\\n\\n• Programming Languages: C, C++, Java, .net, php.\\n• Web Designing: HTML, XML\\n• Operating Systems: Windows […] Windows Server 2003, Linux.\\n• Database: MS Access, MS SQL Server 2008, Oracle 10g, MySql.\"}]},{\"label\":[\"Graduation Year\"],\"points\":[{\"start\":729,\"end\":732,\"text\":\"2016\"}]},{\"label\":[\"College Name\"],\"points\":[{\"start\":675,\"end\":702,\"text\":\"Shivaji University Kolhapur \"}]},{\"label\":[\"Degree\"],\"points\":[{\"start\":631,\"end\":672,\"text\":\"Bachelor of Engg in Information Technology\"}]},{\"label\":[\"Graduation Year\"],\"points\":[{\"start\":625,\"end\":629,\"text\":\"2017\\n\"}]},{\"label\":[\"College Name\"],\"points\":[{\"start\":614,\"end\":622,\"text\":\"CDAC ACTS\"}]},{\"label\":[\"Degree\"],\"points\":[{\"start\":606,\"end\":611,\"text\":\"PG-DAC\"}]},{\"label\":[\"Companies worked at\"],\"points\":[{\"start\":438,\"end\":453,\"text\":\"Cisco Networking\"}]},{\"label\":[\"Email Address\"],\"points\":[{\"start\":104,\"end\":147,\"text\":\"indeed.com/r/Afreen-Jamadar/8baf379b705e37c6\"}]},{\"label\":[\"Location\"],\"points\":[{\"start\":62,\"end\":67,\"text\":\"Sangli\"}]},{\"label\":[\"Name\"],\"points\":[{\"start\":0,\"end\":13,\"text\":\"Afreen Jamadar\"}]}],\"extras\":null,\"metadata\":{\"first_done_at\":1527844872000,\"last_updated_at\":1537724086000,\"sec_taken\":0,\"last_updated_by\":\"BIQNZm4INNfvByMqkaVwVt6OZTv2\",\"status\":\"done\",\"evaluation\":\"CORRECT\"}}\n",
      "\n",
      "# of resumes:\t 701\n"
     ]
    }
   ],
   "source": [
    "myfile = open( path_to_data, \"r\", encoding = \"utf-8\" )\n",
    "\n",
    "imported_data = []\n",
    "\n",
    "for datum in myfile:\n",
    "    \n",
    "    # TODO process data\n",
    "    imported_data.append(datum)\n",
    "\n",
    "myfile.close()\n",
    "\n",
    "# TODO print first line\n",
    "print(\"First line:\\t\", imported_data[0])\n",
    "\n",
    "# TODO print how many resumees were read in\n",
    "print(\"# of resumes:\\t\", len(imported_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "id": "SxQojbZyPrkh"
   },
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "id": "Axwa389baXDN"
   },
   "outputs": [],
   "source": [
    "mapped_data = [ json.loads( datum ) for datum in imported_data  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d2FaAX-xabje",
    "outputId": "d2bb316c-1825-455c-bfb6-7479d6d7a0fe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nr of resumes: 701\n"
     ]
    }
   ],
   "source": [
    "## data conversion method\n",
    "def convert_data(data):\n",
    "    \"\"\"\n",
    "    Creates NER training data in Spacy format from JSON dataset\n",
    "    Outputs the Spacy training data which can be used for Spacy training.\n",
    "    \"\"\"\n",
    "    text = data['content']\n",
    "    entities = []\n",
    "    if data['annotation'] is not None:\n",
    "        for annotation in data['annotation']:\n",
    "            # only a single point in text annotation.\n",
    "            point = annotation['points'][0]\n",
    "            labels = annotation['label']\n",
    "            # handle both list of labels or a single label.\n",
    "            if not isinstance(labels, list):\n",
    "                labels = [labels]\n",
    "            for label in labels:\n",
    "                # dataturks indices are both inclusive [start, end] but spacy is not [start, end)\n",
    "                entities.append((point['start'], point['end'] + 1, label))\n",
    "    return (text, {\"entities\": entities})\n",
    "   \n",
    "## TODO using a loop or list comprehension, convert each resume in mapped_data using the convert function above, \n",
    "## storing the result\n",
    "converted_resumes = [convert_data(resume) for resume in mapped_data]\n",
    "## TODO print the number of resumes in converted resumes \n",
    "print(\"nr of resumes:\", len(converted_resumes) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "id": "GapDFuttaqpk"
   },
   "outputs": [],
   "source": [
    "# TODO filter out the resumees whose entities have no entries.\n",
    "converted_complete_resumees = [complete_resume for complete_resume in converted_resumes if len(complete_resume[1][\"entities\"]) > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1-UWPUvfbo0l"
   },
   "source": [
    "Up until now, you could reuse the code from the previous notebook, now, something new comes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CYhNshfBbl8d",
    "outputId": "b382b82f-3681-48a7-8a5a-869497cb9d51"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<spacy.lang.en.English object at 0x0000014DFCCAB3D0>\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "spacy.prefer_gpu()\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "print(nlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9AqzhVdHb-xO"
   },
   "source": [
    "__nlp__ is Spacy's English language model. For this model, a pretrained NER-model exists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TDHFtX6Lb-CY",
    "outputId": "4739b18a-26dc-4c0d-e6f9-ff1e56e8e79a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('CARDINAL', 'DATE', 'EVENT', 'FAC', 'GPE', 'LANGUAGE', 'LAW', 'LOC', 'MONEY', 'NORP', 'ORDINAL', 'ORG', 'PERCENT', 'PERSON', 'PRODUCT', 'QUANTITY', 'TIME', 'WORK_OF_ART')\n",
      "label:\t GPE \texplanation:\t Countries, cities, states\n",
      "label:\t FAC \texplanation:\t Buildings, airports, highways, bridges, etc.\n",
      "label:\t NORP \texplanation:\t Nationalities or religious or political groups\n"
     ]
    }
   ],
   "source": [
    "ner = nlp.get_pipe('ner')\n",
    "labels = ner.labels\n",
    "print(labels)\n",
    "\n",
    "labels_of_interest = ['GPE', 'FAC', 'NORP']\n",
    "for lbl in labels_of_interest:\n",
    "    print(\"label:\\t\", lbl, \"\\texplanation:\\t\", spacy.explain(lbl))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sBy9Uo20cZH2"
   },
   "source": [
    "__Question 1__: Explain the labels __GPE__, __FAC__, __NORP__.<br>\n",
    "Which of these labels from __ner__ do you think will Spacy recognize in the resumees?<br>\n",
    "__Task 1__: choose a resumee."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4yXayIFffE7F",
    "outputId": "0d4f063b-c30c-43eb-c403-748980090504"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Afreen Jamadar\n",
      "Active member of IIIT Committee in Third year\n",
      "Sangli, Maharashtra - Email me on Indeed: indeed.com/r/Afreen-Jamadar/8baf379b705e37c6\n",
      "I wish to use my knowledge, skills and conceptual understanding to create excellent team\n",
      "environments and work consistently achieving organization objectives believes in taking initiative\n",
      "and work to excellence in my work.\n",
      "WORK EXPERIENCE\n",
      "Active member of IIIT Committee in Third year\n",
      "Cisco Networking -  Kanpur, Uttar Pradesh\n",
      "organized by Techkriti IIT Kanpur and Azure Skynet.\n",
      "PERSONALLITY TRAITS:\n",
      "• Quick learning ability\n",
      "• hard working\n",
      "EDUCATION\n",
      "PG-DAC\n",
      "CDAC ACTS\n",
      "2017\n",
      "Bachelor of Engg in Information Technology\n",
      "Shivaji University Kolhapur -  Kolhapur, Maharashtra\n",
      "2016\n",
      "SKILLS\n",
      "Database (Less than 1 year), HTML (Less than 1 year), Linux. (Less than 1 year), MICROSOFT\n",
      "ACCESS (Less than 1 year), MICROSOFT WINDOWS (Less than 1 year)\n",
      "ADDITIONAL INFORMATION\n",
      "TECHNICAL SKILLS:\n",
      "• Programming Languages: C, C++, Java, .net, php.\n",
      "• Web Designing: HTML, XML\n",
      "• Operating Systems: Windows […] Windows Server 2003, Linux.\n",
      "• Database: MS Access, MS SQL Server 2008, Oracle 10g, MySql.\n",
      "https://www.indeed.com/r/Afreen-Jamadar/8baf379b705e37c6?isid=rex-download&ikw=download-top&co=IN\n"
     ]
    }
   ],
   "source": [
    "# TODO get a single resume text and print it out.\n",
    "restxt = converted_complete_resumees[0][0]\n",
    "## print it out, removing extraneous spaces\n",
    "print(\"\\n\".join(restxt.split('\\n\\n')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we let __nlp__ process that single resumee.<br>\n",
    "__Task 2__: print the results in __doc__. For each result, print the underlying text and the label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "id": "cp961WAxfW8N"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token Afreen Jamadar label PERSON\n",
      "token IIIT Committee label ORG\n",
      "token Third year label DATE\n",
      "token Sangli label ORG\n",
      "token Maharashtra - Email label ORG\n",
      "token IIIT Committee label ORG\n",
      "token Third year label DATE\n",
      "token Cisco Networking -   label ORG\n",
      "token Kanpur label GPE\n",
      "token Uttar label GPE\n",
      "token Techkriti IIT Kanpur label ORG\n",
      "token Azure Skynet label WORK_OF_ART\n",
      "token 2017 label DATE\n",
      "token Bachelor of Engg in Information Technology label ORG\n",
      "token Shivaji University label ORG\n",
      "token Kolhapur label PERSON\n",
      "token Kolhapur label GPE\n",
      "token Maharashtra label ORG\n",
      "token 2016 label DATE\n",
      "token SKILLS label ORG\n",
      "token Less than 1 year label DATE\n",
      "token Less than 1 year label DATE\n",
      "token Linux label PERSON\n",
      "token Less than 1 year label DATE\n",
      "token MICROSOFT label ORG\n",
      "token ACCESS label ORG\n",
      "token Less than 1 year label DATE\n",
      "token MICROSOFT label ORG\n",
      "token Less than 1 year label DATE\n",
      "token C++ label CARDINAL\n",
      "token Java label PERSON\n",
      "token XML label ORG\n",
      "token Operating Systems label ORG\n",
      "token 2003 label DATE\n",
      "token Linux label PERSON\n",
      "token MS Access label ORG\n",
      "token SQL label ORG\n",
      "token 2008 label DATE\n",
      "token 10 label CARDINAL\n",
      "token MySql label PERSON\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(restxt)\n",
    "# TODO  Print the results in doc. For each result, print the text and the label.\n",
    "for ent in doc.ents:\n",
    "    print(\"token\", ent.text, \"label\", ent.label_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xok9PDMplMNe"
   },
   "source": [
    "__Question 2__: How well did Spacy perform at recognizing the labels for this text?<br>\n",
    "When Spacy predicted the labels for this resumee, a pretrained model was used.<br>\n",
    "__Task 3__: print for this resumee the original labels and their corresponding text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V8eIm9lXlznU",
    "outputId": "20998945-3f2b-4c12-91c2-c814c75cbc6b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text:\t indeed.com/r/Afreen-Jamadar/8baf379b705e37c6 label:\t Email Address\n",
      "text:\t https://www.indeed.com/r/Afreen-Jamadar/8baf379b705e37c6?isid=rex-download&ikw=download-top&co=IN label:\t Links\n",
      "text:\t Database (Less than 1 year), HTML (Less than 1 year), Linux. (Less than 1 year), MICROSOFT\n",
      "ACCESS (Less than 1 year), MICROSOFT WINDOWS (Less than 1 year)\n",
      "\n",
      "ADDITIONAL INFORMATION\n",
      "\n",
      "TECHNICAL SKILLS:\n",
      "\n",
      "• Programming Languages: C, C++, Java, .net, php.\n",
      "• Web Designing: HTML, XML\n",
      "• Operating Systems: Windows […] Windows Server 2003, Linux.\n",
      "• Database: MS Access, MS SQL Server 2008, Oracle 10g, MySql. label:\t Skills\n",
      "text:\t 2016 label:\t Graduation Year\n",
      "text:\t Shivaji University Kolhapur  label:\t College Name\n",
      "text:\t Bachelor of Engg in Information Technology label:\t Degree\n",
      "text:\t 2017\n",
      " label:\t Graduation Year\n",
      "text:\t CDAC ACTS label:\t College Name\n",
      "text:\t PG-DAC label:\t Degree\n",
      "text:\t Cisco Networking label:\t Companies worked at\n",
      "text:\t indeed.com/r/Afreen-Jamadar/8baf379b705e37c6 label:\t Email Address\n",
      "text:\t Sangli label:\t Location\n",
      "text:\t Afreen Jamadar label:\t Name\n"
     ]
    }
   ],
   "source": [
    "# TODO print for that resumee the original labels and their corresponding text.\n",
    "\n",
    "labeled_ents = converted_complete_resumees[0][1][\"entities\"]\n",
    "\n",
    "for ent in labeled_ents:\n",
    "    start = ent[0]\n",
    "    end = ent[1]\n",
    "    label = ent[2]\n",
    "\n",
    "    print(\"text:\\t\", restxt[start:end], \"label:\\t\", label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aBHplvL2KHXO"
   },
   "source": [
    "__Question 3__: Compare the performance of the pretrained model __nlp__ and the true labels. Did Spacy perform well? If not, try to explain why.<br>\n",
    "__Task 4__: Remember last homework? You chose three labels. Select all the resumees, in which all three labels appear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wELEGAoEKDwf",
    "outputId": "c4861270-0f2a-45d4-fb03-9819360af42b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gathered 474 training examples\n"
     ]
    }
   ],
   "source": [
    "# TODO fill in your chosen labels in chosen_entity_labels\n",
    "chosen_entity_labels = [ \"Skills\", \"Location\", \"Email Address\"]\n",
    "\n",
    "## this method gathers all resumes which have all of the chosen entites above.\n",
    "def gather_candidates(dataset,entity_labels):\n",
    "    candidates = list()\n",
    "    for resume in dataset:\n",
    "        res_ent_labels = list(zip(*resume[1][\"entities\"]))[2]\n",
    "        if set(entity_labels).issubset(res_ent_labels):\n",
    "            candidates.append(resume)\n",
    "    return candidates\n",
    "\n",
    "training_data = gather_candidates( converted_complete_resumees, chosen_entity_labels )\n",
    "print(\"Gathered {} training examples\".format(len(training_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xSDI7dDDLBbW"
   },
   "source": [
    "__Task 5__: Next, we want to remove all other entities, since we only want to train NER for the three entities in __chosen_entity_labels__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "id": "1DBr_VzxLNqe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oik\n"
     ]
    }
   ],
   "source": [
    "## filter all annotation based on filter list\n",
    "def filter_ents(ents, filter):\n",
    "    filtered = [ent for ent in ents if ent[2] in filter]\n",
    "    return filtered\n",
    "\n",
    "## TODO use method above to remove all but relevant (chosen) entity annotations and store in X variable. X shall contain all\n",
    "## the resumees from training_data, but their entity annotations shall be filtered using the function from above.\n",
    "X = [(datum[0], {\"entities\": filter_ents(datum[1][\"entities\"], chosen_entity_labels)}) for datum in training_data]\n",
    "print(\"oik\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IZpyxtX6NteV"
   },
   "source": [
    "__Task 6__: Some resumees cause trouble. We filter these out with the following lines of code.<br>\n",
    "First, use __add_label__ to add your chosen labels to the __ner__ model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RqwcgW_xX18e",
    "outputId": "06228c50-8bc9-425f-e2d9-dabf6a7de23d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bad sample: overlap\n",
      "bad sample: overlap\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-124-a06d87fd48bb>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     29\u001B[0m   \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     30\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 31\u001B[1;33m     \u001B[0mnlp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mupdate\u001B[0m\u001B[1;33m(\u001B[0m \u001B[1;33m[\u001B[0m \u001B[0mtext\u001B[0m \u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m[\u001B[0m \u001B[0mgold\u001B[0m \u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdrop\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;36m0.3\u001B[0m \u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     32\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     33\u001B[0m   \u001B[1;32mexcept\u001B[0m \u001B[0mException\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\formlos moin\\source\\repos\\saki\\02\\saki_2021_ner\\venv\\lib\\site-packages\\spacy\\language.py\u001B[0m in \u001B[0;36mupdate\u001B[1;34m(self, docs, golds, drop, sgd, losses, component_cfg)\u001B[0m\n\u001B[0;32m    517\u001B[0m             \u001B[0mkwargs\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mcomponent_cfg\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mget\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mname\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m{\u001B[0m\u001B[1;33m}\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    518\u001B[0m             \u001B[0mkwargs\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msetdefault\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"drop\"\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdrop\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 519\u001B[1;33m             \u001B[0mproc\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mupdate\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdocs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mgolds\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0msgd\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mget_grads\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mlosses\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mlosses\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    520\u001B[0m             \u001B[1;32mfor\u001B[0m \u001B[0mkey\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m(\u001B[0m\u001B[0mW\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdW\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mgrads\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mitems\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    521\u001B[0m                 \u001B[0msgd\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mW\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdW\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mkey\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mkey\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mpipes.pyx\u001B[0m in \u001B[0;36mspacy.pipeline.pipes.Tagger.update\u001B[1;34m()\u001B[0m\n",
      "\u001B[1;32mc:\\users\\formlos moin\\source\\repos\\saki\\02\\saki_2021_ner\\venv\\lib\\site-packages\\thinc\\neural\\_classes\\feed_forward.py\u001B[0m in \u001B[0;36mbegin_update\u001B[1;34m(self, X, drop)\u001B[0m\n\u001B[0;32m     44\u001B[0m         \u001B[0mcallbacks\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     45\u001B[0m         \u001B[1;32mfor\u001B[0m \u001B[0mlayer\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_layers\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 46\u001B[1;33m             \u001B[0mX\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0minc_layer_grad\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mlayer\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mbegin_update\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mX\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdrop\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mdrop\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     47\u001B[0m             \u001B[0mcallbacks\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mappend\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0minc_layer_grad\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     48\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\formlos moin\\source\\repos\\saki\\02\\saki_2021_ner\\venv\\lib\\site-packages\\thinc\\api.py\u001B[0m in \u001B[0;36mbegin_update\u001B[1;34m(seqs_in, drop)\u001B[0m\n\u001B[0;32m    293\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mbegin_update\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mseqs_in\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdrop\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m0.0\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    294\u001B[0m         \u001B[0mlengths\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mlayer\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mops\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0masarray\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mlen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mseq\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[0mseq\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mseqs_in\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 295\u001B[1;33m         \u001B[0mX\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mbp_layer\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mlayer\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mbegin_update\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mlayer\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mops\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mflatten\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mseqs_in\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mpad\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mpad\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdrop\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mdrop\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    296\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mbp_layer\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    297\u001B[0m             \u001B[1;32mreturn\u001B[0m \u001B[0mlayer\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mops\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0munflatten\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mX\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mlengths\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mpad\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mpad\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\formlos moin\\source\\repos\\saki\\02\\saki_2021_ner\\venv\\lib\\site-packages\\thinc\\neural\\ops.pyx\u001B[0m in \u001B[0;36mthinc.neural.ops.Ops.flatten\u001B[1;34m()\u001B[0m\n",
      "\u001B[1;32m<__array_function__ internals>\u001B[0m in \u001B[0;36mconcatenate\u001B[1;34m(*args, **kwargs)\u001B[0m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "from spacy.gold import GoldParse \n",
    "\n",
    "# TODO add your labels\n",
    "for label in chosen_entity_labels:\n",
    "    ner.add_label( label )\n",
    "\n",
    "nlp.begin_training()\n",
    "\n",
    "good = []\n",
    "\n",
    "for item in X:\n",
    "  \n",
    "  text = nlp.make_doc( item[ 0 ] )\n",
    "\n",
    "  try:\n",
    "    gold = GoldParse( text, entities = item[ 1 ][ \"entities\" ] )\n",
    "\n",
    "  except ValueError as ve:\n",
    "      sub = str(ve)[:6]\n",
    "      if sub == '[E103]':\n",
    "          print(\"bad sample: overlap\")\n",
    "      else:\n",
    "          print(\"some value error\", ve)\n",
    "      continue\n",
    "  except Exception as e1:\n",
    "    print(\"nok\", e1)\n",
    "    continue\n",
    "  \n",
    "  try:\n",
    "    \n",
    "    nlp.update( [ text ], [ gold ], drop = 0.3 )\n",
    "\n",
    "  except Exception as e:\n",
    "    print(\"ex\", e)\n",
    "    print(\"nok\")\n",
    "\n",
    "  else:\n",
    "\n",
    "    good.append( item )\n",
    "\n",
    "print( \"Number of good samples: \" + str( len( good ) ) )\n",
    "\n",
    "print( \"\" )\n",
    "\n",
    "print( \"\" )\n",
    "\n",
    "print( \"Number of bad sampples: \" + str( len( X ) - len( good ) ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "obZ93SLzyFIe"
   },
   "source": [
    "For a machine learning model, it is essential to be able to generalize. Only a model, that can generalize well is able to process new data in a meaningful way. Therefore, one usually separates the data set into two sets: the training set and the test set. The training set is used to train the model. The test set is used to evaluate the performance of the model on unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k0XVZPTyyh4T"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split( [ item[ 0 ] for item in good ], [ item[ 1 ] for item in good ], test_size = 0.3 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3AcgIty-1otR"
   },
   "source": [
    "__Task 7__: Complete the following code. Shuffle __new_index__. Create the data sets __x_shuffled__ and __y_shuffled__. Use these to create minibatches, iterate over these minibatches, preprocess the data in a given minibatch using __nlp.make_doc__ and __GoldParse__. Employ __nlp.update__ to update the model using these preprocessed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zGLUALLr1rEp"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "nlp.begin_training()\n",
    "\n",
    "new_index = np.arange( len( x_train ) )\n",
    "\n",
    "x_data = np.array( x_train )\n",
    "\n",
    "y_data = np.array( y_train )\n",
    "\n",
    "for i in range( 5 ): # todo waren mal 20\n",
    "\n",
    "  # TODO shuffle new_index\n",
    "  np.random.shuffle(new_index)\n",
    "\n",
    "  x_shuffled = x_data[new_index] #TODO create x_shuffled from x_data by using the shuffled new_index\n",
    "\n",
    "  y_shuffled = y_data[new_index] #TODO create y_shuffled from y_data by using the shuffled new_index\n",
    "\n",
    "  # TODO\n",
    "  # divide the data in x_shuffled and y_shuffled into minibatches of identical size\n",
    "  nr_of_batches = 5\n",
    "  batch_size = int(len(x_shuffled) / nr_of_batches)\n",
    "  minibatches = [(x_shuffled[i*batch_size:(i+1) * batch_size], y_shuffled[i*batch_size:(i+1) * batch_size]) for i in range(nr_of_batches - 1)]\n",
    "  minibatches.append((x_shuffled[(nr_of_batches-1) * batch_size:], y_shuffled[(nr_of_batches-1) * batch_size:]))\n",
    "  # iterate over these minibatches\n",
    "  for x_mini, y_mini in minibatches:\n",
    "    docs = []\n",
    "    golds = []\n",
    "\n",
    "    for j in range(len(x_mini)):\n",
    "        text = str(x_mini[j])\n",
    "        label = y_mini[j][\"entities\"]\n",
    "\n",
    "        doc = nlp.make_doc(text)\n",
    "        gold = GoldParse(doc, entities=label)\n",
    "\n",
    "        \"\"\"\n",
    "        print(\"info on alignment:\")\n",
    "        print(spacy.gold.biluo_tags_from_offsets(doc, label))\n",
    "        \"\"\"\n",
    "\n",
    "        docs.append(doc)\n",
    "        golds.append(gold)\n",
    "    nlp.update( docs, golds, drop = 0.3 )\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PQSe2-BOYlWN"
   },
   "source": [
    "__Question 4__: Why did we shuffle the data?<br> \n",
    "Why did we employ mini batches?<br>\n",
    "Reasearch the term __epoch__ in machine learning. How many epochs of training do we employ?<br>\n",
    "__Task 8__: Next, we choose one resumee and print it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SFGKoHVXY1pL",
    "outputId": "777591ed-d01f-404b-92ec-56ec08456e60"
   },
   "outputs": [],
   "source": [
    "resume_nr = 5\n",
    "resume = good[resume_nr][0]\n",
    "\n",
    "print( resume )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Task 9__: we process this resumee using __nlp__. Print for all items in __doc.ents__ the predicted label and the corresponding text. Then print the correct labels and their corresponding text for that resumee with data from __y_test__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "arAyrdyfbnQR",
    "outputId": "48935714-bda8-4e75-fbe7-a3e39b1ef038"
   },
   "outputs": [],
   "source": [
    "doc = nlp( resume )\n",
    "\n",
    "# TODO\n",
    "# print for all the items in doc.ents the predicted label and the corresponding text\n",
    "\n",
    "original_entities = good[resume_nr][1][\"entities\"]\n",
    "print(\"original entities\")\n",
    "for item in original_entities:\n",
    "    print(item, \"=\", resume[item[0]:item[1]])\n",
    "print(\"\")\n",
    "print(\"predicted entities\")\n",
    "for item in doc.ents:\n",
    "    print(item.label_, \"=\", item.text)\n",
    "\n",
    "# TODO\n",
    "# print the correct labels and their corresponding text for that resumee with data from y_test\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DTjZw4RVeB87"
   },
   "source": [
    "__Question 5__: What labels did the model predict correctly?<br> \n",
    "Where appeared problems?<br> \n",
    "How can you explain the problems?<br>\n",
    "__Question 6__: We can evaluate the performance of the model using 4 metrics: the __Accuracy__, the __Precision__, the __Recall__ and __F1__.<br>\n",
    "Inform yourself on these metrics. How are they defined? Explain the concept of __True Positive__, __True Negative__, __False Positive__ and __False Negative__. Use these to define  the __Accuracy__, the __Precision__, the __Recall__ and __F1__, and also give the formula for each of these.<br>\n",
    "__Task 10__: Complete the following code. Call __make_bilou_df__ with a resume from the test set and store result in __bilou_df__ variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "W5M-ArsDtzFd",
    "outputId": "80cd4596-2ead-45a9-bf5a-3214e6ffd926",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from spacy.gold import biluo_tags_from_offsets\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "## returns a pandas dataframe with tokens, prediction, and true (Gold Standard) annotations of tokens\n",
    "def make_bilou_df(nlp,resume):\n",
    "    \"\"\"\n",
    "    param nlp - a trained spacy model\n",
    "    param resume - a resume from our train or test set\n",
    "    \"\"\"\n",
    "    doc = nlp(resume[0])\n",
    "    bilou_ents_predicted = biluo_tags_from_offsets(doc, [(ent.start_char,ent.end_char,ent.label_)for ent in doc.ents])\n",
    "    bilou_ents_true = biluo_tags_from_offsets(doc, [(ent[0], ent[1], ent[2]) for ent in resume[1][\"entities\"]])\n",
    "\n",
    "    \n",
    "    doc_tokens = [tok.text for tok in doc]\n",
    "    bilou_df = pd.DataFrame()\n",
    "    bilou_df[\"Tokens\"] =doc_tokens\n",
    "    bilou_df[\"Tokens\"] = bilou_df[\"Tokens\"].str.replace(\"\\\\s+\",\"\") \n",
    "    bilou_df[\"Predicted\"] = bilou_ents_predicted\n",
    "    bilou_df[\"True\"] = bilou_ents_true\n",
    "    return bilou_df\n",
    "\n",
    "## TODO call method above with a resume from test set and store result in bilou_df variable.\n",
    "bilou_df = make_bilou_df( nlp, (x_test[0], y_test[0]) )\n",
    "display(bilou_df)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inform yourself on the [BILUO](https://spacy.io/usage/linguistic-features#accessing-ner) scheme.<br>\n",
    "__Question 7__: Why do you think is it better to tag entities using this scheme (consider names of humans, descriptions of items in a shop)?<br>\n",
    "__Task 11__: employ pandas dataframe api to get a subset where predicted and true labels are the same. Compute the accuracy using the formula you researched above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HWRI3IfluPD7",
    "outputId": "95b35d82-edbf-46a4-a741-a4a6d3d64508"
   },
   "outputs": [],
   "source": [
    "## TODO bilou_df is a pandas dataframe. Use pandas dataframe api to get a subset where predicted and true are the same. \n",
    "same_df = bilou_df[bilou_df[\"Predicted\"] == bilou_df[\"True\"]]\n",
    "## TODO compute the accuracy \n",
    "accuracy = len(same_df) / len(bilou_df)\n",
    "\n",
    "print(\"Accuracy on one resume: \",accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1P4EgefioE-Q"
   },
   "source": [
    "The __accuracy__ is not 100%. Therefore, we want to have a look at those tokens, where the predicted and the true value differ.<br>\n",
    "__Task 12__: create a dataframe diff_df where the predicted values and the true values differ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 225
    },
    "id": "DdHFX1cMn-r6",
    "outputId": "b00c8f4c-90be-4314-a1ff-a7f93b6ed6dd"
   },
   "outputs": [],
   "source": [
    "# TODO create a dataframe diff_df where the predicted values and the true values differ\n",
    "diff_df = bilou_df[bilou_df[\"Predicted\"] != bilou_df[\"True\"]]\n",
    "display(diff_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OWvyiCokonUI"
   },
   "source": [
    "Since we only considered one resumee, we now make this comparison for the whole test set.<br>\n",
    "__Task 13__: Complete the following code for the computation of the overall accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bidqT9GjovAg",
    "outputId": "c912f5a7-27b0-4cb1-fe15-7b54aaf565cc"
   },
   "outputs": [],
   "source": [
    "doc_accuracy = []\n",
    "\n",
    "for i in range( len( x_test ) ):\n",
    "\n",
    "  resume = x_test[i], y_test[i]\n",
    "\n",
    "  bilou_df = make_bilou_df(nlp,resume)\n",
    "\n",
    "  same_df = bilou_df[bilou_df[\"Predicted\"] == bilou_df[\"True\"]]\n",
    "\n",
    "  doc_accuracy.append( len(same_df) / len(bilou_df) )\n",
    "\n",
    "total_acc = np.mean( doc_accuracy )\n",
    "print(\"Accuracy: \",total_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cL7GE3RQpzbg"
   },
   "source": [
    "So we got an __accuracy__ of about 90% on average. This is quite good considering, that we only considered about 300 cases for training.<br>\n",
    "__Task 14__: Next, we want to find out, what the model did, when it went wrong. We only consider 5 resumees.<br>\n",
    "Complete the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y02Pkxriq8GM",
    "outputId": "122a9e41-bd60-4e87-e04a-5c12d1264d0c"
   },
   "outputs": [],
   "source": [
    "for i in range( 5 ):\n",
    "\n",
    "  resume = good[(i+1) * 10]\n",
    "\n",
    "  bilou_df = make_bilou_df(nlp,resume)\n",
    "\n",
    "  difference_df = bilou_df[bilou_df[\"Predicted\"] != bilou_df[\"True\"]]\n",
    "\n",
    "  # TODO: print, where the labels from Spacy and the annotation differ. Print the text, the predicted and the true labels.\n",
    "  print(\"resume at position\", (i+1) * 10, \"in 'good'\")\n",
    "  print(\"---TEXT---\")\n",
    "  print(resume[0])\n",
    "  print(\"\")\n",
    "  print(\"---DIFF---\")\n",
    "  print(difference_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0r21TdjPrt7P"
   },
   "source": [
    "__Question 8__: What was predicted, when the prediction differed from the true label?<br>\n",
    "What do you think is necessary for computing the accuracy on token level?<br> \n",
    "What is the advantage of computing the accuracy on token level?<br>\n",
    "__Task 15__: Complete the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 186
    },
    "id": "GZGOVg2U20V1",
    "outputId": "b0d900fd-abf1-49a7-dd15-641049f688c1"
   },
   "outputs": [],
   "source": [
    "## TODO cycle through chosen_entity_labels and calculate metrics for each entity using test data\n",
    "data = []\n",
    "for label in chosen_entity_labels:\n",
    "    ## variables to store results for all resumes for one entity type\n",
    "    true_positives = 0\n",
    "    false_positives = 0\n",
    "    false_negatives = 0\n",
    "    for i in range( len( x_test ) ):\n",
    "        ## use make_bilou_df on each resume in our test set, and calculate for each entity true and false positives,\n",
    "        ## and false negatives. \n",
    "\n",
    "        resume = x_test[i], y_test[i]\n",
    "        \n",
    "        tres_df = make_bilou_df(nlp,resume)\n",
    "\n",
    "        ## calculate true false positives and false negatives for each resume\n",
    "        all_tp = tres_df[tres_df[\"True\"] == tres_df[\"Predicted\"]]\n",
    "        tp = len(all_tp[all_tp[\"True\"].str.contains(label)])\n",
    "\n",
    "        all_fp = tres_df[tres_df[\"True\"] != tres_df[\"Predicted\"]]\n",
    "        fp = len(all_fp[all_fp[\"Predicted\"].str.contains(label)])\n",
    "\n",
    "        all_fn = tres_df[tres_df[\"True\"] != tres_df[\"Predicted\"]]\n",
    "        fn = len(all_fn[all_fn[\"True\"].str.contains(label)])\n",
    "\n",
    "        ## aggregate result for each resume to totals\n",
    "        true_positives = true_positives + tp\n",
    "        false_positives = false_positives + fp\n",
    "        false_negatives = false_negatives + fn\n",
    "    \n",
    "    print(\"For label '{}' tp: {} fp: {} fn: {}\".format(label,true_positives,false_positives,false_negatives))\n",
    "    \n",
    "    ## TODO Use the formulas you learned to calculate metrics and print them out\n",
    "    ## also: prevent division by zero without raising errors. Explain your choice\n",
    "    eps = .000001\n",
    "    precision = true_positives / (true_positives + false_positives + eps)\n",
    "    recall = true_positives / (true_positives + false_negatives + eps)\n",
    "    f1 = 2 * (precision * recall)/(precision + recall + eps)\n",
    "    \n",
    "    row = [label,precision,recall,f1]\n",
    "    data.append(row)\n",
    "\n",
    "## make pandas dataframe with metrics data. Use the chosen entity labels as an index, and the metric names as columns. \n",
    "metric_df = pd.DataFrame( data, columns = [ \"Label\", \"Precision\", \"Recall\", \"F1\" ] )\n",
    "display(metric_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rMKMlJswAqUZ"
   },
   "source": [
    "__Question 9__: Explain from these statistics how well __nlp__ performs.<br>\n",
    "__Task 16__: Compute for each metric (Precision, Recall, F1) the mean values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5_gEZTQy5KTr",
    "outputId": "9f098559-a028-4ee5-dbba-f34909c244bd"
   },
   "outputs": [],
   "source": [
    "for label in [ \"Precision\", \"Recall\", \"F1\" ]:\n",
    "    \n",
    "    val = sum(metric_df[label]) / len(metric_df)\n",
    "    print(\"mean\", label, \"is at\", \"%.2f\" % val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Q1rvvhfBoN9"
   },
   "source": [
    "__Question 10__: What do you learn, when you compare the performance of the model on the token level with the performance of the model on the global level from above?<br>\n",
    "Next, we prepare data for flair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7vkzBYIlDbLe"
   },
   "outputs": [],
   "source": [
    "train = [ [ x_train[ i ], y_train[ i ] ] for i in range( len( x_train ) ) ]\n",
    "\n",
    "test = [ [ x_test[ i ], y_test[ i ] ] for i in range( len( x_test ) ) ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Task 17__: Complete the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "id": "NBfdFr1qNKBv"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-89-260bb16f66ef>:19: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  bilou_df[\"Tokens\"] = bilou_df[\"Tokens\"].str.replace(\"\\\\s+\",\"\")\n"
     ]
    }
   ],
   "source": [
    "# prepare data\n",
    "training_data_as_bilou = [make_bilou_df(nlp,res) for res in train]\n",
    "\n",
    "test_data_as_bilou = [make_bilou_df(nlp,res) for res in test]\n",
    "\n",
    "\n",
    "# set up paths\n",
    "path_to_training_file = os.getcwd() + \"/training_data.csv\"\n",
    "\n",
    "path_to_test_file = os.getcwd() + \"/test_data.csv\"\n",
    "\n",
    "\n",
    "\n",
    "# make sure, that if the corresponding files exist, they are emptied\n",
    "if os.path.isfile( path_to_training_file ):\n",
    "\n",
    "  open( path_to_training_file, \"w\" ).close()\n",
    "\n",
    "if os.path.isfile( path_to_test_file ):\n",
    "\n",
    "  open( path_to_test_file, \"w\" ).close()\n",
    "\n",
    "\n",
    "# open empty files\n",
    "training_file = open( path_to_training_file, \"a\", encoding = \"utf-8\" )\n",
    "    \n",
    "test_file = open( path_to_test_file, \"a\", encoding = \"utf-8\" )\n",
    "\n",
    "\n",
    "for item in training_data_as_bilou:\n",
    "    for i, row in item.iterrows():\n",
    "        text = row[\"Tokens\"]\n",
    "        if text in [\"\", \" \", \"\\n\"]:\n",
    "            continue\n",
    "      # TODO remove all tokens like \"\", \" \", \"\\n\" by ignoring them\n",
    "      # for all other tokens do the following:\n",
    "      # create a string s: s = token + \" \" + label + \"\\n\"\n",
    "        s = text + \" \" + (row[\"True\"] if row[\"True\"] != \"-\" else \"0\") + \"\\n\"\n",
    "          # if the label is \"-\", then write s = token + \" O\\n\"\n",
    "          #\n",
    "          # write this newly created string to file\n",
    "        training_file.write(s)\n",
    "          # if this newly created string contains \".\", then also write a\n",
    "          # newline to file that only contains \"\\n\"\n",
    "        if \".\" in s:\n",
    "            training_file.write(\"\\n\")\n",
    "  #\n",
    "  # Using this scheme, each line in the resulting files corresponds either to an empty line or a token.\n",
    "  # Flair assembles a block of nonempty lines into a sentence. Therefore, the empty line\n",
    "  # is a signal for Flair that the current sentence is finished. Therefore, we extracted\n",
    "  # the whitespaces above.\n",
    "\n",
    "for item in test_data_as_bilou:\n",
    "    # TODO the same as above.\n",
    "     for i, row in item.iterrows():\n",
    "        text = row[\"Tokens\"]\n",
    "        if text in [\"\", \" \", \"\\n\"]:\n",
    "            continue\n",
    "        s = text + \" \" + (row[\"True\"] if row[\"True\"] != \"-\" else \"0\") + \"\\n\"\n",
    "     test_file.write(s)\n",
    "     if \".\" in s:\n",
    "        test_file.write(\"\\n\")\n",
    "\n",
    "\n",
    "training_file.close()\n",
    "\n",
    "test_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-V5trnr4gXa9"
   },
   "source": [
    "Start Flair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "630pwinWhXWo",
    "outputId": "e984243a-85fa-428f-837d-22448a1ed294"
   },
   "outputs": [],
   "source": [
    "# pip install flair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UKoPUsQGgaEE",
    "outputId": "d73c2577-6870-42a2-d776-919b40424376"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 12:06:04,668 Reading data from C:\\Users\\formlos moin\\source\\repos\\saki\\02\\SAKI_2021_NER\n",
      "2021-05-25 12:06:04,668 Train: C:\\Users\\formlos moin\\source\\repos\\saki\\02\\SAKI_2021_NER\\training_data.csv\n",
      "2021-05-25 12:06:04,669 Dev: None\n",
      "2021-05-25 12:06:04,670 Test: C:\\Users\\formlos moin\\source\\repos\\saki\\02\\SAKI_2021_NER\\test_data.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-130-54291343cab4>:20: DeprecationWarning: Call to deprecated function (or staticmethod) load_column_corpus. (Use 'flair.datasets' instead.) -- Deprecated since version 0.4.1.\n",
      "  corpus = NLPTaskDataFetcher.load_column_corpus(data_folder,column_format=columns,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus: 7383 train + 820 dev + 58 test sentences\n"
     ]
    }
   ],
   "source": [
    "from flair.data import Corpus\n",
    "from flair.data_fetcher import NLPTaskDataFetcher\n",
    "\n",
    "\n",
    "# your training file name\n",
    "data_folder = os.getcwd() \n",
    "\n",
    "train_file = \"training_data.csv\"\n",
    "\n",
    "# your training file name\n",
    "test_file = \"test_data.csv\"\n",
    "\n",
    "# when we wrote the data files, each row was either empty to signal the end\n",
    "# of a sentence to Flair, or the line contained a token, a white space and a label.\n",
    "# In the next line, we assign, that the token is the \"text\", and that the label is \n",
    "# \"ner\" label\n",
    "columns =  {0: 'text', 1: 'ner'}\n",
    "\n",
    "## Now load our csv into flair corpus\n",
    "corpus = NLPTaskDataFetcher.load_column_corpus(data_folder,column_format=columns,\n",
    "                                               train_file=train_file,\n",
    "                                               test_file=test_file)\n",
    "print(corpus)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Resumee_NER_Spacy.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}