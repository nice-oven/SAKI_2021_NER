{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j4CyQGFg4haX"
   },
   "source": [
    "Flair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zRxrYeEV4gVL",
    "outputId": "f0cfc709-5454-4874-e731-a360414942cb"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "j6Toer0O450W"
   },
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fH08d9VJ5Ife"
   },
   "outputs": [],
   "source": [
    "# os.chdir( \"/content/gdrive/MyDrive/flair\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ug6zYGQ85KbW",
    "outputId": "cf3d9dc3-21f6-49fb-de34-12061bfa761e"
   },
   "outputs": [],
   "source": [
    "# pip install flair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "HdwhJEXL54Aw"
   },
   "outputs": [],
   "source": [
    "from flair.data import Corpus\n",
    "from flair.datasets import WIKINER_ENGLISH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OXGeR8FJ8Bsj"
   },
   "source": [
    "Next, we create __wikiner_corpus__, an instance of the class __Corpus__.<br>\n",
    "Read [here](https://github.com/flairNLP/flair/blob/master/resources/docs/TUTORIAL_6_CORPUS.md) the documentation of __Corpus__.<br>\n",
    "__Question 1__: explain, what the __WIKINER__ corpus is.<br>\n",
    "Then, we create __tag_dictionary__ which is an __BILUO__-__NER__-encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4hqOdEn7565N",
    "outputId": "34b7677e-6385-4a7a-9e8c-3c3d6cdcd484"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 12:08:19,343 https://raw.githubusercontent.com/dice-group/FOX/master/input/Wikiner/aij-wikiner-en-wp3.bz2 not found in cache, downloading to C:\\Users\\FORMLO~1\\AppData\\Local\\Temp\\tmpp9f8b15p\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6208404/6208404 [00:06<00:00, 1031014.93B/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 12:08:25,538 copying C:\\Users\\FORMLO~1\\AppData\\Local\\Temp\\tmpp9f8b15p to cache at C:\\Users\\formlos moin\\.flair\\datasets\\wikiner_english\\aij-wikiner-en-wp3.bz2\n",
      "2021-05-25 12:08:25,551 removing temp file C:\\Users\\FORMLO~1\\AppData\\Local\\Temp\\tmpp9f8b15p\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 12:08:30,530 Reading data from C:\\Users\\formlos moin\\.flair\\datasets\\wikiner_english\n",
      "2021-05-25 12:08:30,531 Train: C:\\Users\\formlos moin\\.flair\\datasets\\wikiner_english\\aij-wikiner-en-wp3.train\n",
      "2021-05-25 12:08:30,532 Dev: None\n",
      "2021-05-25 12:08:30,533 Test: None\n",
      "Corpus: 11514 train + 1279 dev + 1422 test sentences\n",
      "[b'<unk>', b'O', b'S-ORG', b'B-ORG', b'I-ORG', b'E-ORG', b'S-PER', b'B-PER', b'E-PER', b'S-MISC', b'S-LOC', b'B-LOC', b'E-LOC', b'I-LOC', b'I-PER', b'B-MISC', b'I-MISC', b'E-MISC', b'<START>', b'<STOP>']\n"
     ]
    }
   ],
   "source": [
    "# 1. get the corpus\n",
    "wikiner_corpus: Corpus = WIKINER_ENGLISH().downsample(0.1)\n",
    "print(wikiner_corpus)\n",
    "# 3. make the tag dictionary from the corpus\n",
    "tag_dictionary = Corpus.make_tag_dictionary( wikiner_corpus, tag_type='ner')\n",
    "print(tag_dictionary.idx2item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2W4UKrp-7uw8",
    "outputId": "6301807d-1160-4de9-f9ce-4163f4ae1256"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: \"Tensions between Algeria and Morocco in relation to the Western Sahara have put great obstacles in the way of tightening the Maghreb Union , which was nominally established in 1989 but carried little practical weight with its coastal neighbors .\"   [− Tokens: 40  − Token-Labels: \"Tensions <NNS> between <IN> Algeria <NNP/S-LOC> and <CC> Morocco <NNP/S-LOC> in <IN> relation <NN> to <TO> the <DT> Western <JJ/B-LOC> Sahara <NNP/E-LOC> have <VBP> put <VBN> great <JJ> obstacles <NNS> in <IN> the <DT> way <NN> of <IN> tightening <VBG> the <DT> Maghreb <NNP/B-ORG> Union <NNP/E-ORG> , <,> which <WDT> was <VBD> nominally <RB> established <VBN> in <IN> 1989 <CD> but <CC> carried <VBD> little <JJ> practical <JJ> weight <NN> with <IN> its <PRP$> coastal <JJ> neighbors <NNS> . <.>\"]\n",
      "Tensions <NNS> between <IN> Algeria <NNP/S-LOC> and <CC> Morocco <NNP/S-LOC> in <IN> relation <NN> to <TO> the <DT> Western <JJ/B-LOC> Sahara <NNP/E-LOC> have <VBP> put <VBN> great <JJ> obstacles <NNS> in <IN> the <DT> way <NN> of <IN> tightening <VBG> the <DT> Maghreb <NNP/B-ORG> Union <NNP/E-ORG> , <,> which <WDT> was <VBD> nominally <RB> established <VBN> in <IN> 1989 <CD> but <CC> carried <VBD> little <JJ> practical <JJ> weight <NN> with <IN> its <PRP$> coastal <JJ> neighbors <NNS> . <.>\n"
     ]
    }
   ],
   "source": [
    "print(wikiner_corpus.train[73])\n",
    "print(wikiner_corpus.train[73].to_tagged_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Br-nRr9Q8g1T"
   },
   "source": [
    "Ok, above, we loaded a corpus, a collection of texts, and with this collection the annotation of these texts.<br>\n",
    "Next, we load the data, we prepared using Spacy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LD2GaEJN76XT",
    "outputId": "205754e8-33a5-4871-ed2e-2fb400675456"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 12:09:28,241 Reading data from C:\\Users\\formlos moin\\source\\repos\\saki\\02\\SAKI_2021_NER\n",
      "2021-05-25 12:09:28,241 Train: C:\\Users\\formlos moin\\source\\repos\\saki\\02\\SAKI_2021_NER\\training_data.csv\n",
      "2021-05-25 12:09:28,242 Dev: None\n",
      "2021-05-25 12:09:28,243 Test: C:\\Users\\formlos moin\\source\\repos\\saki\\02\\SAKI_2021_NER\\test_data.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-6-0043053970f9>:8: DeprecationWarning: Call to deprecated function (or staticmethod) load_column_corpus. (Use 'flair.datasets' instead.) -- Deprecated since version 0.4.1.\n",
      "  corpus: Corpus = NLPTaskDataFetcher.load_column_corpus(data_folder, columns,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus: 7383 train + 820 dev + 58 test sentences\n",
      "[b'<unk>', b'O', b'U-Location', b'0', b'B-Skills', b'I-Skills', b'L-Skills', b'U-Email', b'B-Email', b'L-Email', b'B-Location', b'I-Location', b'L-Location', b'I-Email', b'U-Skills', b'<START>', b'<STOP>']\n"
     ]
    }
   ],
   "source": [
    "from flair.data_fetcher import NLPTaskDataFetcher\n",
    "\n",
    "downsample = 1.0 # 1.0 is full data, try a much smaller number like 0.01 to test run the code\n",
    "data_folder = os.getcwd()\n",
    "columns = {0: 'text', 1: 'ner'}\n",
    "\n",
    "# 1. get the corpus\n",
    "corpus: Corpus = NLPTaskDataFetcher.load_column_corpus(data_folder, columns,\n",
    "                                                             train_file='training_data.csv',\n",
    "                                                             test_file='test_data.csv',\n",
    "                                                           dev_file=None).downsample(downsample)\n",
    "print(corpus)\n",
    "\n",
    "# 3. make the tag dictionary from the corpus\n",
    "tag_dictionary = corpus.make_tag_dictionary(tag_type='ner')\n",
    "print(tag_dictionary.idx2item)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question 2__: what is the difference between __tag_dictionary__ created in the cell above, and __tag_dictionary__ created before that.<br>\n",
    "Next, we take the first sentence from the test data, and annotate this sentence using __to_tagged_string__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Nie8-FX99JB7",
    "outputId": "6f049a50-a642-4fb3-8fb1-5bfdb1a22477"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.indeed.com/r/Anshika-S/305eb39072429b30?isid=rex-download&ikw=download-top&co=IN <0>\n"
     ]
    }
   ],
   "source": [
    "for sent in corpus.test:\n",
    "  print(sent.to_tagged_string())\n",
    "  break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question 3__: Why is not every word annotated?<br>\n",
    "How do you explain the difference to the result from __to_tagged_string__ applied to one sentence from the wiki ner corpus?"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Flair_NER_1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}